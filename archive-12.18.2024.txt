from typing import Annotated
from langchain_community.tools import TavilySearchResults
from langchain_core.messages import AIMessage
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import ToolNode, tools_condition
from langchain_groq import ChatGroq
import os

#short term memory and internet search tool
os.environ['TAVILY_API_KEY'] = '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'
os.environ['LANGCHAIN_TRACING_V2'] ="true"
os.environ['LANGCHAIN_ENDPOINT']="https://api.smith.langchain.com"
os.environ['LANGCHAIN_API_KEY']="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"
os.environ['LANGCHAIN_PROJECT']="pr-overcooked-forever-55"

# Create a checkpointer (memory saver) to save the state of the conversation
memory = MemorySaver()

# Define the state for the conversation
class State(TypedDict):
    messages: Annotated[list, add_messages]


graph_builder = StateGraph(State)

# Set up the tools (e.g., TavilySearchResults, for example)
tool = TavilySearchResults(max_results=2)  # Example tool
tools = [tool]

# Set up the LLM with tools enabled
llm = ChatGroq(groq_api_key=os.getenv("GROQ_API _KEY"), model_name="llama3-groq-8b-8192-tool-use-preview")
llm_with_tools = llm.bind_tools(tools)

# Define the chatbot node
def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}


graph_builder.add_node("chatbot", chatbot)

# Use LangGraph's prebuilt ToolNode for handling tool invocations
tool_node = ToolNode(tools=[tool])
graph_builder.add_node("tools", tool_node)

# Add conditional edges to determine if tools are being called
graph_builder.add_conditional_edges(
    "chatbot",
    tools_condition,
)

# Ensure the flow returns to the chatbot after using a tool
graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge(START, "chatbot")

# Compile the graph with memory checkpointer to save conversation state
graph = graph_builder.compile(checkpointer=memory)

def stream_graph_updates(user_input, config):
    events = graph.stream(
        {"messages": [("user", user_input)]}, config, stream_mode="values"
    )
    for event in events:
        # Only print the AI's response (not the user's input)
        if "messages" in event and event["messages"]:
            last_message = event["messages"][-1]
            if isinstance(last_message, AIMessage):
                print("Assistant:", last_message.content)


# Start the conversation by selecting a thread ID for memory persistence
config = {"configurable": {"thread_id": "1"}}

# Example interaction
while True:
    try:
        user_input = input("User: ")
        if user_input.lower() in ["quit", "exit", "q"]:
            print("Goodbye!")
            break

        # Stream the updates to the conversation
        stream_graph_updates(user_input, config)
    except:
        pass